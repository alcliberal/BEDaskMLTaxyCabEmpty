{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the OBD SDD BE!\n",
    "\n",
    "Today, the goal is to understand how a distributed system can be useful when dealing with medium to large scale data sets.  \n",
    "We'll see that Dask start to be nice as soon as the Data we need to process doesn't quite fit in memory, but also if we\n",
    "need to launch several computations in parallel.\n",
    "\n",
    "In this evaluation, you will:\n",
    "- Use Dask to read and understand the several gigabytes input dataset in a interactive way,\n",
    "- Preprocess the data in a distributed way: cleaning it up and adding some useful features,\n",
    "- Launch some model training that can be parallelized,\n",
    "- Reduce the dataset and train more accurate models on less Data,\n",
    "- Do an hyper parameter search to find the best model on a small sample of Data.\n",
    "\n",
    "In order to run and fill this notebook, you'll need to first deploy a Dask enabled Kubernetes cluster as seen last week. So please use the Kubernetes_DaskHub notebook for the steps to do it. __Be careful, it has been updated to use a Docker image containing ML Libraries since the last time!__.\n",
    "\n",
    "Once the Jupyterhub is up, you can clone the OBD directory from a Jupyterlab terminal to get this notebook, and select the default kernel.\n",
    "```\n",
    "git clone https://github.com/SupaeroDataScience/OBD.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "It is some statistics about NY Taxi cabs. \n",
    "\n",
    "See https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview, or https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data.\n",
    "        \n",
    "The goal of this evaluation will be to generate a model using machine learning technics that will predict the fare amount\n",
    "of a taxi ride given the other input parameters we have.\n",
    "\n",
    "The model will be evaluated using the Root mean squared error algorithm:  \n",
    "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to analyze the Data using Kaggles'start-up code\n",
    "\n",
    "As an introduction, we'll use Kaggle starters'code to get some insights on the data set and\n",
    "computations we'll do and measure pandas library (non parallelized access) performance.\n",
    "\n",
    "See https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model where this comes from.\n",
    "\n",
    "On our data set (train and test available in gs://obd-dask), we'll see that with Kaggle method, we don't obtain a really good evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data with pandas\n",
    "\n",
    "We're reading only about 20% from the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "train_df =  pd.read_csv('gs://obd-dask/train.csv', nrows = 10_000_000)\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing dataset, adding some feature and droping null values\n",
    "\n",
    "Let's see if we can see some links between passenger count and fare amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df.groupby(train_df.passenger_count).fare_amount.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe adding some feature about the distance of the trip could be a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there some undefined values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick analyze on new features and clean outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get training features and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\n",
    "\n",
    "train_X = get_input_matrix(train_df)\n",
    "train_y = np.array(train_df['fare_amount'])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a simple linear model using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction on our test set and measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.read_csv('gs://obd-dask/test.csv')\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X = get_input_matrix(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = np.matmul(test_X, w).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_ref = test_df.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y_ref, test_y_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so about 5,23$ of RMSE, this is not that bad... But we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "### Some questions on this first Analysis\n",
    "\n",
    "- What is the most expensive part of the analysis, the one that takes the most time (see the %%time we used above)?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Try to load the whole dataset with Pandas and comment.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing our data set using dask\n",
    "\n",
    "Dask will help us processing all the input data set. It is really useful when input data is too big to fit in memory. In this case, it can stream the computation by data blocs one one computer, or distribute the computation on several computers.\n",
    "\n",
    "This is what we'll do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start an appropriate sized Dask cluster for our analysis\n",
    "\n",
    "We'll need a Dask cluster to pre process the data and distribute some learning, the following code starts one in our K8S infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "# Use values stored in your local configuration (recommended)\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(worker_cores=1, worker_memory=3.4)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Please click on the Dashboard link above, it will help you a lot!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "cluster.scale(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch some computation, what about Pi ?\n",
    "\n",
    "Just to check our cluster is working!\n",
    "\n",
    "We'll use Dask array, a Numpy extension for this, taht we'll use later on for the Machine Learning part of this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.array as da\n",
    "\n",
    "sample = 10_000_000_000  # <- this is huge!\n",
    "xxyy = da.random.uniform(-1, 1, size=(2, sample))\n",
    "norm = da.linalg.norm(xxyy, axis=0)\n",
    "summ = da.sum(norm <= 1)\n",
    "insiders = summ.compute()\n",
    "pi = 4 * insiders / sample\n",
    "print(\"pi ~= {}\".format(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, access the data of our BE using Dask\n",
    "\n",
    "We'll use Dask Dataframe, an distributed version of Pandas Dataframe.\n",
    "\n",
    "See https://docs.dask.org/en/latest/dataframe.html.\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "So instead of using Pandas to load the dataset, just use the equivalent dask method from dask.dataframe.\n",
    "\n",
    "- Fill the following cell with the appropriate code to read the data using Dask.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "### Some questions about this data loading\n",
    "\n",
    "- That was fast for several gigabytes, wasn't it? Why is this, what did we do?\n",
    "- Why the return dataframe looks empty?\n",
    "- See the number of partitions described above? What does it correspond to?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little warm up: Analyzing our data to better understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- First, how many records do we have? (hint, in python, len() works for almost any object).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What did happend when counting record of our Dask dataframe? Remember with the Spark tutorial: transformations and actions... Same kind of concepts exist in Dask. Just look at the Dask Dashboard!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Compare the time of this computation to the time of loading a subset of the Dataset with Pandas. What is fast enough considering the number of worker we have?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Why was it faster than the count records operation above? What did wee read?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Let's compute the mean of the fare given the passenger_count, as we've done with Pandas. Please fill the blank. (hint: don't forget the compute() call)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, ever seen a cab with more than 200 people??\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- This is a bit slow, much more than with Pandas, why? Which part of the computation is slow, look at the Dashboard to see the name of the tasks. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- How cloud we optimize the next computations ? Where will be the data at the end?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Look at the Dashboard at what is happening beind the scene.\n",
    "At the end, try again the computation:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here, the same computation on fare_amount mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "### Some other questions to practice\n",
    "\n",
    "- Can you see a correlation between the fare amount and the dropoff latitude? Answer by doing a dask dataframe computation.\n",
    "\n",
    "First you'll need to round the dropoff latitude to have some sort of categories using Series.round() function.\n",
    "\n",
    "Then, just group_by this new colon to have some answer (and don't forget to compute to get the results).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this don't give a lot of insights, but it looks like we've got some strange values somewhere!\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- Let's just have a look of non extreme values, so probably some records at the middle of the results.\n",
    "We need first to sort the resulting series by index befor lookin at the middle of it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this is not really useful, but it's an exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Do you think we could parallelize things better for any of our computation or data access?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some preprocessing of our data to clean it up and add some features\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- You'll need to do the same operations as in pandas, we just need to call compute when needing a result, and not compute when building our dataframe transformations.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "#### Cleaning up\n",
    "\n",
    "- Is there some null values in our data?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Yep! We must get rid of them...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- As with Pandas above, add the lattitude and longitude distance vector with a function call\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at our Dataframe to check things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Now let's quickly plot a subset of our travel vector features to see its distribution. Use dask.dataframe.sample() to get about five percent of the rows, and get it back with compute and plot like with Pandas\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, looks like we have some strange values here: more than 1000° of distance... There's a problem somewhere.\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Just get rid of the extreme values, we should keep inside the city wall or so. Like with Pandas.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What is triggering the computation in the examples above?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- you can do another plot like above with the filtered values if you like.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's see some statistics on our Dataset. The describe() function inherited from Pandas compute a lot of statistics on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Are there some values that still looks odd to you in here?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model in a distributed way\n",
    "\n",
    "Let's begin with a linear model that we can distributed with Dask ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our feature vectors\n",
    "\n",
    "Here again define a method so that we can use it later for our test set evaluation.\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Just do the same as with the Pandas example by defining a get_input_matrix(df) function. But this time you'll generate a dask array using to_dask_array(length=True) method on the dataframe. You should do a method that generate the X input features dask array, and also the same with y training results. You can do just one method that return both. \n",
    "- It is a good idea to persist() arrays in memory in or after the call.\n",
    "- This time, we'll add the feature 'passenger_count' in addition to the distance vectors.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the values, and display train_X to have some insights of its size and chunking scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_input_matrix(train_df)\n",
    "# or train_X =\n",
    "# train_y =\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed training a Linear model\n",
    "\n",
    "Be careful, this can take time, try first with few iterations (max_iter = 5).\n",
    "\n",
    "see https://ml.dask.org/glm.html  \n",
    "and https://ml.dask.org/modules/generated/dask_ml.linear_model.LinearRegression.html#dask_ml.linear_model.LinearRegression\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Train a LinearRegression model from dask_ml.linear_model on our inputs\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we should load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dd.read_csv('gs://obd-dask/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding our features to the test set and getting our feature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X, test_y = get_input_matrix(test_df)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the score method inherited from Scikit learn, it gives some hints on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just get the numpy arrays for computing final score, this is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X.compute()\n",
    "test_y = test_y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the RMSE\n",
    "\n",
    "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, lr.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What RMSE did you get? Compare it to the Pandas only computation.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Why is this model not really good?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dask to scale computation on Hyper Parameter Search\n",
    "\n",
    "As seen above, Dask is well suited to distribute Data and learn a model on a big Data set. However, not all the models can be train in parallel on sub chunks of Data. See https://scikit-learn.org/stable/computing/scaling_strategies.html for the compatible models of Sickit learn for example.\n",
    "\n",
    "Dask can also be used to train several model in parallel on small datasets, this is what we'll try now.\n",
    "\n",
    "We will just take a sample of the training set, and try to learn several models with different hyper parameters, and find the best one.\n",
    "\n",
    "Dask Hyper parameter search : https://ml.dask.org/hyper-parameter-search.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll take a small subset of the Data, 5% is a maximum if we want to avoir memory issues on our workers and have appropriate training times. You can try with less if the results are still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a sample of the input data, get it as pandas dataframe\n",
    "train_sample_df = train_df.sample(frac=0.05, random_state=270120)\n",
    "# Get feature vectors out of it\n",
    "train_sample_X, train_sample_y = get_input_matrix(train_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize things, we can also change the type of the features to more appropriate and small types.\n",
    "\n",
    "We also need to use Numpy arrays, so we'll gather the result from Dask to local variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_X = train_sample_X.astype('float32').compute()\n",
    "train_sample_y = train_sample_y.astype('float32').compute()\n",
    "train_sample_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What size is our dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(train_sample_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 32MB, this is still quite a big dataset for standard machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- Now, just use dask hyper parameter search Dask API to distribute the search. You can either use joblib integration with Sklearn or dask_ml directly. Be careful: do not use model too long to train, and limit their complexity at first or the combinations of hyper parameters you'll use. Hint, start first with a simple LinearModel like SGDRegressor and not more than 10 iterations per model.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- So how does the result compare to distributed leaning with a linear model? On all the dataset?\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Can you do better with Random forest? Caution: use limited trees, small number of estimators < 5 and max_depth < 40...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What do you observe when training RandomForest tree on Dask parallelization Dashboard? Can you explain why there are so many tasks?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Did you get better results with RandomForest? Why ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "# Extend this notebook\n",
    "    \n",
    "Try to do better!\n",
    "\n",
    "- Add new features to the input Data using Dask Dataframes, or clean it better. Reapply the learning above with these new features. Do you get better results? Some suggestions for a better leaning:\n",
    "  - Max passenger count of 208, maybe we should ignore this value? Rides with 0 passengers? Try to drop some data.\n",
    "  - Apply some normalisation or regularization or other feature transformation? See https://ml.dask.org/preprocessing.html.\n",
    "  - There are 0m rides?\n",
    "  - Negative fare amount?? Drop some data.\n",
    "  - Maybe the hour of the day, or the month, has some impact on fares? Try to add features. See https://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes for some hints on how to do this.\n",
    "  - Maybe try to find a way to use the start and drop off locations?\n",
    "- Improve the model parameters or find a better one. Try using this time dask_ml HyperbandSearchCV. See https://ml.dask.org/hyper-parameter-search.html#basic-use. You can use it for example with https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor.\n",
    "\n",
    "</span>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pangeo 2020-12",
   "language": "python",
   "name": "pangeo_202012"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
